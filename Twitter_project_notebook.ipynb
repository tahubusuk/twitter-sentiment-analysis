{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b16ec0d",
   "metadata": {
    "id": "8b16ec0d"
   },
   "source": [
    "# ML Course 2023 |  Sentiment Analysis in Twitter Challenge\n",
    "You can check the updated leaderboard in this [link](https://nimble-hellebore-184.notion.site/ML-Course-2023-Sentiment-Analysis-in-Twitter-Challenge-966b041e7aec4f2eabbc8dc33d64b871)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EJfbR5fCE1eN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39634,
     "status": "ok",
     "timestamp": 1682427886056,
     "user": {
      "displayName": "Isabel Valera",
      "userId": "14662300919014123166"
     },
     "user_tz": -120
    },
    "id": "EJfbR5fCE1eN",
    "outputId": "72a2c4bb-195c-4b19-f8e2-8e4a0a49925a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install tueplots==0.0.5\n",
    "!pip3 install sentence-transformers==2.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6077c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install WordCloud\n",
    "!pip3 install seaborn\n",
    "!pip3 install -U imbalanced-learn\n",
    "!pip3 install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5eaed7",
   "metadata": {
    "id": "2f5eaed7"
   },
   "source": [
    "# Load Tweets\n",
    "\n",
    "The dataframe of tweets contain the following columns:\n",
    "\n",
    "- `id`: The unique identifier of the tweet\n",
    "- `text`: The content of the tweet\n",
    "- `type`: The type of tweet, which can be 'tweet', 'quoted', 'retweeted' or 'quoted__replied_to'\n",
    "- `author_id`: The unique identifier of the author of the tweet\n",
    "- `possibly_sensitive`: A boolean value indicating whether the tweet contains sensitive content\n",
    "- `retweet_count`: The number of times the tweet has been retweeted\n",
    "- `quote_count`: The number of times the tweet has been quoted\n",
    "- `reply_count`: The number of times the tweet has been replied to\n",
    "- `like_count`: The number of times the tweet has been liked\n",
    "- `followers_count`: The number of followers of the author of the tweet\n",
    "- `following_count`: The number of accounts the author of the tweet is following\n",
    "- `tweet_count`: The total number of tweets made by the author of the tweet\n",
    "- `listed_count`: The number of lists the author of the tweet is a member of\n",
    "- `score_compound`:  A numerical value ranging from -1 to 1 indicating the overall sentiment of the tweet, where -1 represents  negative sentiment and 1 represents positive sentiment. **This is the target variable for the regression task.**\n",
    "- `sentiment`: A categorical variable indicating the sentiment of the tweet, which can be 'negative', 'neutral' or 'positive'. **This is the target variable for the classification task.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762536aa",
   "metadata": {
    "id": "762536aa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tueplots import bundles\n",
    "\n",
    "plt.rcParams.update(bundles.icml2022())\n",
    "import tueplots.constants.color.palettes as tue_palettes\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6b0bb",
   "metadata": {
    "id": "2fc6b0bb"
   },
   "outputs": [],
   "source": [
    "team_id = '1' #put your team id here\n",
    "split = 'test_1' # replace by 'test_2' for FINAL submission\n",
    "\n",
    "df = pd.read_csv('tweets_train.csv') # we are subsampling 100 training data\n",
    "df_test = pd.read_csv(f'tweets_{split}.csv') # we are subsampling 200 test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321d106",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1682351179439,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "c321d106",
    "outputId": "52c430b5-e5fe-4412-d579-687609eb9dee"
   },
   "outputs": [],
   "source": [
    "df[df.type == 'tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cee5aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1682351182266,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "57cee5aa",
    "outputId": "85c2cee9-9c1b-4b8e-daba-0f52e9c088f6"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fefb4",
   "metadata": {
    "id": "1e7fefb4"
   },
   "source": [
    "# Pre-process tweets\n",
    "\n",
    "The following are the preprocessing steps we followed to get the `words` column from the original tweet, which corresponds to the `text` column of the dataframe.\n",
    "\n",
    "- Remove punctuations, special characters, mentions, links, and numbers from the tweets.\n",
    "- Convert all the tweets to lowercase.\n",
    "- Tokenize the tweets into individual words.\n",
    "- Remove stop words, such as \"and\", \"the\", \"a\", etc.\n",
    "- Perform stemming or lemmatization on the remaining words to convert them to their base form.\n",
    "- Filter out any words that occur infrequently in the corpus to reduce the dimensionality of the data.\n",
    "- Create a bag of words representation of the tweets, where each tweet is represented as a vector of word frequencies.\n",
    "\n",
    "\n",
    "**Note:** Lemmatization is a process in natural language processing where words are reduced to their base form, or lemma. This is done by removing inflections, such as pluralization or verb conjugation, and converting the word to its dictionary form. The result of this process is a word that is more easily recognizable, and can be used to improve the accuracy of NLP models, such as the LDA model. By lemmatizing the words in a corpus of text, the dimensionality of the data is reduced, and the relationships between words become clearer, making it easier to identify patterns and themes within the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c35dbf",
   "metadata": {
    "id": "56c35dbf"
   },
   "outputs": [],
   "source": [
    "df['words_str'] = df['words'].apply(lambda words: ' '.join(eval(words)))\n",
    "df_test['words_str'] = df_test['words'].apply(lambda words: ' '.join(eval(words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcfff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['words_str'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9b1e6",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb285231",
   "metadata": {},
   "source": [
    "The following processes are used to reduce the dimension of the variables and to normalize all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44071b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_label = df.drop(['sentiment', 'words', 'words_str', 'text', 'type', 'possibly_sensitive'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_wo_label = df_test.drop(['words', 'words_str', 'text', 'type', 'possibly_sensitive'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43375a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d3f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21310a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to do it every time we train and validate the model\n",
    "df_norm = preprocessing.StandardScaler().fit_transform(df_wo_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db44461",
   "metadata": {
    "id": "3db44461"
   },
   "source": [
    "# Visualize content of the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2Uzfq4IwXwz7",
   "metadata": {
    "id": "2Uzfq4IwXwz7"
   },
   "source": [
    "Join all of the preprocessed tweets together and create a world cloud of them to see most frequently used words among all tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a7f05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1682351186097,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "601a7f05",
    "outputId": "460ff91f-a7e1-4ffa-e140-d8be48292f77"
   },
   "outputs": [],
   "source": [
    "long_string = ','.join(list(df['words_str'].values))\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(font_path = 'C:/Users/bodhi/AppData/Local/Microsoft/Windows/Fonts/times.ttf', background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1071a69",
   "metadata": {
    "id": "c1071a69"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7Yyjw1lMYI76",
   "metadata": {
    "id": "7Yyjw1lMYI76"
   },
   "source": [
    "**In this part, we will visualize the distribution of these possible sentiments in our dataset.**\n",
    "\n",
    "Each tweet in our dataset have one of three sentiments (`sentiment`):\n",
    "\n",
    "*   Positive\n",
    "*   Neutral\n",
    "*   Negative\n",
    "\n",
    "Also, each tweet has a continous score (`score_compound`) between [-1,1] where -1 corresponds to negative and 1 corresponds to a positive sentinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c122718",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1682351191253,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "9c122718",
    "outputId": "f5132424-6fde-43ca-a3f2-f340874a9e1c"
   },
   "outputs": [],
   "source": [
    "df_pos = df[df.sentiment == 'positive']\n",
    "df_neu = df[df.sentiment == 'neutral']\n",
    "df_neg = df[df.sentiment == 'negative']\n",
    "\n",
    "\n",
    "num_total = len(df)\n",
    "num_pos = len(df_pos)\n",
    "num_neu = len(df_neu)\n",
    "num_neg = len(df_neg)\n",
    "\n",
    "print(f\"Num. positive tweets: {num_pos} ({num_pos/num_total*100:.2f}%)\")\n",
    "print(f\"Num. negative tweets: {num_neg} ({num_neg/num_total*100:.2f}%)\")\n",
    "print(f\"Num. neutral tweets: {num_neu} ({num_neu/num_total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ae151",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682351193098,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "827ae151",
    "outputId": "668a61a0-b870-4d0a-eace-d5e7f4f30fc2"
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "labels = []\n",
    "labels.append(f\"Positive ({num_pos/num_total*100:.2f}%)\")\n",
    "labels.append(f\"Neutral ({num_neu/num_total*100:.2f}%)\")\n",
    "labels.append(f\"Negative ({num_neg/num_total*100:.2f}%)\")\n",
    "\n",
    "sizes = [num_pos, num_neu, num_neg]\n",
    "\n",
    "colors = [f\"#{i}\" for i in tue_palettes.high_contrast[:3]]\n",
    "\n",
    "_ = ax.pie(sizes,colors=colors, startangle=90)\n",
    "# plt.style.use(default’)\n",
    "ax.legend(labels,\n",
    "          loc='upper center', \n",
    "          bbox_to_anchor=(1.23, 1.0), \n",
    "          fancybox=True, \n",
    "          shadow=True)\n",
    "\n",
    "ax.set_title(\"Sentiment Analysis\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85bd0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1682351193917,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "4c85bd0e",
    "outputId": "82da026e-2e11-4b2f-a983-9c3014834d88"
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "sns.countplot(x=df.sentiment, palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad02231",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1682351194486,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "9ad02231",
    "outputId": "1a684bdc-604f-46b3-b763-acc4cb4a0100"
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "sns.violinplot(data=df, x='sentiment', y='score_compound', palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ff4ce",
   "metadata": {},
   "source": [
    "# Check the correlation between data and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4894a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = pd.DataFrame(np.squeeze(df_norm), columns=list(df_wo_label.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfdeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spearman because the data is not normally distributed\n",
    "correlation = df_norm.corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d51886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5), dpi =100)\n",
    "sns.heatmap(correlation,annot=True,fmt=\".2f\", linewidth=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3242f",
   "metadata": {},
   "source": [
    "the correlation between each categories and the score compound is not very strong, but maybe it could help a little bit on our result later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baca5db",
   "metadata": {},
   "source": [
    "# Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58407382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = PCA(n_components = 0.95, whiten = True).fit(df_norm.drop(['score_compound'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b65fe",
   "metadata": {},
   "source": [
    "to get 95% of the variance, we need 9 dimension, which is just the total -1. I do not think using PCA will be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2682c3",
   "metadata": {},
   "source": [
    "# PLOT that may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d21e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef459fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77914864",
   "metadata": {
    "id": "77914864"
   },
   "source": [
    "# Obtain the text embeddings\n",
    "\n",
    "When working with natural language processing tasks, such as text classification, it is common to use word embeddings to represent the meaning of words and sentences. Word embeddings are dense vectors that capture the semantic relationships between words in a way that allows for easier processing by machine learning algorithms.\n",
    "\n",
    "The process of creating word embeddings involves training a neural network on a large corpus of text data. However, pre-trained word embeddings are readily available online and can be downloaded and used in your projects. See a complete list of pre-trained models [here](https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md). \n",
    "\n",
    "\n",
    "**Note:** When working with pre-trained models, it is important to keep in mind the computational resources required to generate the embeddings. Depending on the size of the model and the amount of text data being processed, generating embeddings may take a significant amount of time. Therefore, it is advisable to save the embeddings locally once they have been generated, to avoid the need to re-generate everytime you may want to make changes in the model (but not in the embedding).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce70f9",
   "metadata": {
    "id": "27ce70f9"
   },
   "outputs": [],
   "source": [
    "# name = 'stsb-distilbert-base'\n",
    "# name = 'all-mpnet-base-v2'\n",
    "name = 'stsb-mpnet-base-v2'\n",
    "# name = 'bert-base-nli-mean-tokens'\n",
    "# name = 'average_word_embeddings_komninos'\n",
    "model = SentenceTransformer(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d3eb9",
   "metadata": {
    "id": "203d3eb9"
   },
   "outputs": [],
   "source": [
    "sentences = list(df.words_str.values)\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "np.save('stsb_mpnet_base_v2_embeddings_all.npy', sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36397e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = np.load(\"stsb_mpnet_base_v2_embeddings_all.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d59e9",
   "metadata": {},
   "source": [
    "Aside from the sentence embeddings, we can try to use another nlp method to vectorize the words with \"Bag of Words\" method and \"Gram\" method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9601aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030559ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = {\n",
    "    'words11g.pkl': None,\n",
    "    'words11g100.pkl': None,\n",
    "    'words11g500.pkl': None,\n",
    "    'words11g1000.pkl': None,\n",
    "    'words11g2500.pkl': None,\n",
    "    'words11g5000.pkl': None,\n",
    "    'words22g100.pkl' : None,\n",
    "    'words22g500.pkl' : None,\n",
    "    'words22g1000.pkl' : None,\n",
    "    'words22g2500.pkl' : None,\n",
    "    'words22g5000.pkl' : None,\n",
    "    'words22g10000.pkl' : None,\n",
    "    'words22g20000.pkl' : None,\n",
    "    'words22g25000.pkl' : None,\n",
    "    'words22g30000.pkl' : None,\n",
    "    'words22g40000.pkl' : None,\n",
    "    'words22g50000.pkl' : None,\n",
    "    'words22g60000.pkl' : None,\n",
    "    'words22g70000.pkl' : None,\n",
    "    'words22g80000.pkl' : None,\n",
    "    'words33g.pkl' : None,\n",
    "    'words33g100.pkl' : None,\n",
    "    'words33g500.pkl' : None,\n",
    "    'words33g2500.pkl' : None,\n",
    "    'words33g1000.pkl' : None,\n",
    "    'words33g5000.pkl' : None,\n",
    "    'words33g10000.pkl' : None,\n",
    "    'words33g25000.pkl' : None,\n",
    "    'words33g50000.pkl' : None,\n",
    "    'words33g75000.pkl' : None,\n",
    "    'words12g.pkl' : None,\n",
    "    'words12g100.pkl' : None,\n",
    "    'words12g500.pkl' : None,\n",
    "    'words12g1000.pkl' : None,\n",
    "    'words12g2500.pkl' : None,\n",
    "    'words12g5000.pkl' : None,\n",
    "    'words12g10000.pkl' : None,\n",
    "    'words12g25000.pkl' : None,\n",
    "    'words12g50000.pkl' : None,\n",
    "    'words12g75000.pkl' : None,\n",
    "    'words23g.pkl' : None,\n",
    "    'words23g100.pkl' : None,\n",
    "    'words23g500.pkl' : None,\n",
    "    'words23g1000.pkl' : None,\n",
    "    'words23g2500.pkl' : None,\n",
    "    'words23g5000.pkl' : None,\n",
    "    'words23g10000.pkl' : None,\n",
    "    'words23g25000.pkl' : None,\n",
    "    'words23g50000.pkl' : None,\n",
    "    'words23g75000.pkl' : None,\n",
    "    'words23g100000.pkl' : None,\n",
    "    'words23g125000.pkl' : None,\n",
    "    'words23g150000.pkl' : None,\n",
    "    'words13g.pkl' : None,\n",
    "    'words13g100.pkl' : None,\n",
    "    'words13g500.pkl' : None,\n",
    "    'words13g1000.pkl' : None,\n",
    "    'words13g2500.pkl' : None,\n",
    "    'words13g5000.pkl' : None,\n",
    "    'words13g10000.pkl' : None,\n",
    "    'words13g25000.pkl' : None,\n",
    "    'words13g50000.pkl' : None,\n",
    "    'words13g75000.pkl' : None,\n",
    "    'words13g100000.pkl' : None,\n",
    "    'words13g125000.pkl' : None,\n",
    "    'words13g150000.pkl' : None,\n",
    "    \n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddab51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b36400e",
   "metadata": {},
   "source": [
    "We will analyze this. First we make few comparisons between BoW with 1-gram, 2-gram, 3-gram, and the union of each categories and also we will look at the top 10% words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437a70e",
   "metadata": {},
   "source": [
    "this has more than 16k unique words, I think we will analyze using 100, 500, 1000, 2500, 5000 top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_pickle(file_name):\n",
    "    with open('count_vectorizer/'+ file_name, 'wb') as f:\n",
    "        pickle.dump(count_vector[file_name], f)\n",
    "        \n",
    "def load_pickle(file_name):\n",
    "    with open('count_vectorizer/'+ file_name, 'rb') as f:\n",
    "        count_vector[file_name] = pickle.load(f)\n",
    "\n",
    "\n",
    "for key in count_vector.keys():\n",
    "    try:\n",
    "        load_pickle(key)\n",
    "    except:\n",
    "        ngram = (int(key[5]), int(key[6]))\n",
    "        max_feature = key[key.find('g') + 1:key.find('.')]\n",
    "        try:\n",
    "            count_vector[key] = CountVectorizer(ngram_range = ngram, max_features = int(max_feature)).fit_transform(df['words_str'])\n",
    "        except:\n",
    "            count_vector[key] = CountVectorizer(ngram_range = ngram).fit_transform(df['words_str'])\n",
    "        finally:\n",
    "            save_pickle(key)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f65d98",
   "metadata": {},
   "source": [
    "we see that we have a lot of unique words. This of course eats a lot of computational power, so we will analyze it further with 10%, 25% of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86306b14",
   "metadata": {},
   "source": [
    "we will use linear regression to analyze this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561616d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(strategy, model, X, y):\n",
    "    sum_rmse = 0\n",
    "    length = 0\n",
    "    for (train, test) in strategy.split(X, y):\n",
    "#         do i need to normalize the input ? I do not think so ?\n",
    "        \n",
    "#         fitting the model\n",
    "        reg = model.fit(X[train], y[train])\n",
    "        y_pred = reg.predict(X[test])\n",
    "        \n",
    "#         calculating the accuracy\n",
    "        rmse = np.sqrt(skm.mean_squared_error(y[test], y_pred))\n",
    "        sum_rmse += rmse\n",
    "        length += 1\n",
    "    return sum_rmse/length\n",
    "#     print(\"\\n strategy = \", strategy, \"\\n model = \", model, \"\\n avg_rmse = \", sum_rmse / length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strategy for model selection\n",
    "kf5 = KFold(n_splits = 5)\n",
    "kf10 = KFold(n_splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression(fit_intercept=True, copy_X=True, n_jobs=None, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07faa152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse_vector = dict()\n",
    "for key in count_vector:\n",
    "    rmse = test_model(kf10, lr, count_vector[key], df['score_compound'])\n",
    "    rmse_vector[key] = rmse\n",
    "rmse_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83822101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just example, in reality i will not use this.\n",
    "# make data\n",
    "x = list(rmse_vector.keys())\n",
    "y = rmse_vector.values()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.rcParams[\"figure.figsize\"] = (100,30)\n",
    "# ax.stem(x, y)\n",
    "markerline, stemlines, baseline = ax.stem(x, y)\n",
    "plt.setp(stemlines, 'linewidth', 30)\n",
    "plt.savefig('res.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOP 5 RMSE \\n\")\n",
    "for e in sorted(rmse_vector, key=rmse_vector.get)[:5]:\n",
    "    print(e + \" = \" + str(rmse_vector[e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d4c75",
   "metadata": {},
   "source": [
    "trying to use TF-IDF to these  5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer as tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fcdbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = dict()\n",
    "\n",
    "def save_pickle(file_name):\n",
    "    with open('tfidf/' + file_name, 'wb') as f:\n",
    "        pickle.dump(tfidf_vector[file_name], f)\n",
    "        \n",
    "def load_pickle(file_name):\n",
    "    with open('tfidf/' + file_name, 'rb') as f:\n",
    "        tfidf_vector[file_name] = pickle.load(f)\n",
    "\n",
    "for key in count_vector.keys():\n",
    "    try:\n",
    "        load_pickle(key)\n",
    "    except:\n",
    "        tfidf_vector[key] = tfidf().fit_transform(count_vector[key])\n",
    "        save_pickle(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87063e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4b6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse_tfidf_vector = dict()\n",
    "for key in tfidf_vector:\n",
    "    rmse = test_model(kf10, lr, tfidf_vector[key], df['score_compound'])\n",
    "    rmse_tfidf_vector[key] = rmse\n",
    "rmse_tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4f582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOP 5 RMSE \\n\")\n",
    "for e in sorted(rmse_tfidf_vector, key=rmse_tfidf_vector.get)[:5]:\n",
    "    print(e + \" = \" + str(rmse_tfidf_vector[e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5bd266",
   "metadata": {},
   "source": [
    "# problem is that how to incorporate another dimension so that the weight is good ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df_norm.drop(['score_compound'], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words1g1000.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = np.append(dfa, words12g75000.toarray(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_model(kf10, lr, dfa, df['score_compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90fb4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test_model(kf10, lr, sentence_embeddings, df['score_compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sgd = linear_model.SGDRegressor(alpha = 0.001,\n",
    " epsilon= 0.5,\n",
    " learning_rate= 'adaptive',\n",
    " loss= 'squared_error',\n",
    " penalty= 'l2',\n",
    " tol= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_model(kf10, sgd, sentence_embeddings, df['score_compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ee0a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test_model(kf10, sgd, words12g75000_tfidf, df['score_compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dffe8ac",
   "metadata": {},
   "source": [
    "if we have time, let's try to make all of them.\n",
    "but for now, we will pick words12g75000_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5e59b",
   "metadata": {},
   "source": [
    "# Trying to test with many regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b1650",
   "metadata": {},
   "source": [
    "we will use the top 5 tfidf and Bag of Words for testing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422163a",
   "metadata": {},
   "source": [
    "another problem to note is the imbalance of the data. Maybe try to balance it first by making more data points? Or pruning them.\n",
    "\n",
    "using SMOTE iirc, we can do it but for categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3210ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.linear_model import Lasso, Perceptron\n",
    "from sklearn.linear_model import BayesianRidge as BR\n",
    "from sklearn.linear_model import SGDRegressor as SGD\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor as RNR, KNeighborsRegressor as KNR\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "model = {\n",
    "    'lr' : LR(n_jobs = -1),\n",
    "    'lasso' : Lasso(),\n",
    "    'sgd': SGD(),\n",
    "#     'br' : BR(), have to be dense matrix, am lazy to do it\n",
    "    'svr' : SVR(),\n",
    "    'knr' : KNR(n_jobs = -1),\n",
    "#     'dtr' : DTR(),\n",
    "    'xgb': xgboost.XGBRegressor(n_jobs = -1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ea8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_model_vector = dict()\n",
    "\n",
    "for key in model.keys():\n",
    "    rmse_score = dict()\n",
    "    for e in sorted(rmse_vector, key=rmse_vector.get)[:3]:\n",
    "        rmse = test_model(kf10, model[key], count_vector[e], df['score_compound'])\n",
    "        rmse_score[e] = rmse\n",
    "    rmse_model_vector[key] = rmse_score\n",
    "rmse_model_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tfidf_model_vector = dict()\n",
    "for key in model.keys():\n",
    "    rmse_score = dict()\n",
    "    for e in sorted(rmse_tfidf_vector, key=rmse_tfidf_vector.get)[:3]:\n",
    "        rmse = test_model(kf10, model[key], tfidf_vector[e], df['score_compound'])\n",
    "        rmse_score[e] = rmse\n",
    "    rmse_tfidf_model_vector[key] = rmse_score\n",
    "rmse_tfidf_model_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2db151",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in model.keys():\n",
    "    print(test_model(kf10, model[key], sentence_embeddings, df['score_compound']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb4907",
   "metadata": {},
   "source": [
    "we will take xgb and lr and try to enhance the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96841033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_func(y_true, y_pred):\n",
    "    return Math.sqrt(pow((y_true - y_pred), 2) / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69304aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBRegressor(n_jobs = -1, verbosity = 3, eval_metric = 'rmse', tree_method = 'gpu_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b040d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {\n",
    "#     'n_estimators' : [750, 1000, 1250]\n",
    "    'max_depth': [3, 5,6,7, 9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ffdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = GridSearchCV(xgb, xgb_param, n_jobs = -1, cv = 10, error_score = 'raise', scoring = metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.fit(count_vector['words12g.pkl'], df['score_compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2323d4",
   "metadata": {
    "id": "9c2323d4"
   },
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GjxnefI-aSOZ",
   "metadata": {
    "id": "GjxnefI-aSOZ"
   },
   "source": [
    "In this part, we will solve an linear regression task to predict our target `score_compound`, i.e. continous sentiment score of tweets, using our features which are encodings of the tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HTijDT6YctxD",
   "metadata": {
    "id": "HTijDT6YctxD"
   },
   "outputs": [],
   "source": [
    "#define some functions for plotting purposes\n",
    "\n",
    "def plot_y_continous(y, bins=10, show=True, title=None):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    _ = ax.hist(y, bins=bins)\n",
    "    if isinstance(title, str):\n",
    "        ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    if show: plt.show()\n",
    "\n",
    "def plot_scatter(x, y,  show=True, x_label=None, y_label=None,  title=None):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    _ = ax.scatter(x,y)\n",
    "    if isinstance(title, str):\n",
    "        ax.set_title(title)\n",
    "    if isinstance(x_label, str):\n",
    "        ax.set_xlabel(x_label)\n",
    "    if isinstance(y_label, str):\n",
    "        ax.set_ylabel(y_label)\n",
    "    plt.tight_layout()\n",
    "    if show: plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-tij9jWO4p_n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682351209849,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "-tij9jWO4p_n",
    "outputId": "e5d7781d-037a-48e1-eaab-14a6e7c7705f"
   },
   "outputs": [],
   "source": [
    "#create X (feature matrix) and y (targets)\n",
    "X = sentence_embeddings\n",
    "y = df.score_compound.values\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912afa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c24f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682351209850,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "027c24f9",
    "outputId": "b5a1b9ca-70f3-46db-bd36-9fb45c165f41"
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plot_y_continous(y, bins=20, title='Histogram of Target variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2_CBqwiigmaF",
   "metadata": {
    "id": "2_CBqwiigmaF"
   },
   "source": [
    "In its simplest form, predictions of a linear regression model can be summarized as\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w}^T \\mathbf{x} = f(\\mathbf{x},\\mathbf{w})\n",
    "$$\n",
    "\n",
    "which can be optimized the cost functions \n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{*}=\\underset{\\mathbf{w}}{\\arg \\min } \\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-f\\left(\\mathbf{x}_{i}, \\mathbf{w}\\right)\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377a1d1",
   "metadata": {
    "id": "5377a1d1"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, svm, tree, metrics\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingGridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215770dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5ec4f",
   "metadata": {},
   "source": [
    "# Using K-Fold strategy to achieve better generalization (5 and 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(strategy, model, X, y):\n",
    "    sum_rmse = 0\n",
    "    length = 0\n",
    "    for (train, test) in strategy.split(X, y):\n",
    "        reg = model.fit(X[train], y[train])\n",
    "        y_pred = reg.predict(X[test])\n",
    "        rmse = np.sqrt(skm.mean_squared_error(y[test], y_pred))\n",
    "        sum_rmse += rmse\n",
    "        length += 1\n",
    "    print(\"\\n strategy = \", strategy, \"\\n model = \", model, \"\\n avg_rmse = \", sum_rmse / length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05394360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question: should we normalize the data? for now, I do not think that we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strategy for model selection\n",
    "kf5 = KFold(n_splits = 5)\n",
    "kf10 = KFold(n_splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model that we wanted to try\n",
    "\n",
    "# 1. linear regression (example)\n",
    "lr = linear_model.LinearRegression(fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "\n",
    "# 1. Lasso\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "# 2. SVM\n",
    "svm = svm.SVR()\n",
    "\n",
    "# 3. SGD \n",
    "sgd = linear_model.SGDRegressor()\n",
    "\n",
    "# 4. Decision tree\n",
    "dt = tree.DecisionTreeRegressor()\n",
    "\n",
    "# 5. neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cde956",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_param = {\n",
    "    'alpha': [1, 0.5, 0.1, 1e-2, 1e-3],\n",
    "    'norm': [False, True]    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gscv = HalvingGridSearchCV(nb, param_grid = nb_param, n_jobs = -1, cv = kf10, scoring = metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9df6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gscv.fit(X_train,z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906722db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_param = {\n",
    "    'loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    'tol': [ 1e-3, 1e-2, 1e-4],\n",
    "    'epsilon': [0.5, 0.25, 0.1, 0.05, 0.01],\n",
    "    'learning_rate': ['adaptive', 'optimal', 'invscaling'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_gscv = HalvingGridSearchCV(sgd, param_grid = sgd_param, n_jobs = -1, cv = kf10, scoring = metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3835b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_gscv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00991623",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd_gscv.predict(X_val)\n",
    "rmse = np.sqrt(skm.mean_squared_error(y_val, y_pred))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param = {'max_depth':                 [2, 4, 8, 16, 32, 64, 128, 256],\n",
    "            'min_samples_split' :        [2, 4, 8, 16, 32, 64],\n",
    "            'min_samples_leaf' :         [2, 4, 8, 16, 32, 64],\n",
    "            'min_weight_fraction_leaf' : [0, 1e-3, 1e-4, 1e-5],\n",
    "            'min_impurity_decrease' :    [0, 1e-3, 1e-4, 1e-5]           \n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bcf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_gscv = HalvingGridSearchCV(dt, param_grid = dt_param, n_jobs = -1, cv = kf5, scoring = metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_gscv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg_gscv.predict(X_val)\n",
    "rmse = np.sqrt(skm.mean_squared_error(y_val, y_pred))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JzZwQFApeUBd",
   "metadata": {
    "id": "JzZwQFApeUBd"
   },
   "outputs": [],
   "source": [
    "#split X and y for training and validation purposes\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_trains, X_vals, z_train, z_val = train_test_split(X, z, test_size=0.2, random_state=42)\n",
    "\n",
    "datasets = [\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val]\n",
    "]\n",
    "\n",
    "#create our linear regression model\n",
    "# reg = linear_model.LinearRegression(fit_intercept=True, copy_X=True, n_jobs=None, positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7255102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0971d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, z_res = ros.fit_resample(X_train, z_train)\n",
    "y_res, z_res = ros.fit_resample(y_train.reshape((6400,1)), z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, z_res = SMOTE().fit_resample(X_train, z_train)\n",
    "##lanjut sini\n",
    "y_res, z_res = SMOTE().fit_resample(y_train.reshape((6400,1)), z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, z_res = SMOTEENN().fit_resample(X_train, z_train)\n",
    "y_res, z_res = SMOTEENN().fit_resample(y_train.reshape((6400,1)), z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res.reshape(6299,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(X_res)) != len(X_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3236f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac23a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = df.sentiment.values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(z)\n",
    "z =le.transform(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9886d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split X and y for training and validation purposes\n",
    "#it is split with categorical features because we want to use SMOTE first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TarmeeGneDFb",
   "metadata": {
    "id": "TarmeeGneDFb"
   },
   "source": [
    "Now fit a linear regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154442a",
   "metadata": {
    "id": "0154442a"
   },
   "outputs": [],
   "source": [
    "reg = reg.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9ff00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1682351233221,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "dba9ff00",
    "outputId": "41edacb2-2034-477b-a382-ca41c06d5aea"
   },
   "outputs": [],
   "source": [
    "# Evaluate our predictor quantitatively\n",
    "for split_name, dataset in zip(['train', 'valididation'], datasets):\n",
    "    X_i, y_i = dataset\n",
    "    y_pred = reg.predict(X_i)\n",
    "\n",
    "    rmse = np.sqrt(skm.mean_squared_error(y_i, y_pred))\n",
    "    print(f'\\nSplit: {split_name}')\n",
    "    print(f\"\\tRMSE: {rmse:.2f}\")\n",
    "    mae = skm.mean_absolute_error(y_i, y_pred)\n",
    "    print(f\"\\tMAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4cf20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1682351236116,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "fcd4cf20",
    "outputId": "87a3adfb-9677-4300-de94-1348eb73cd2e"
   },
   "outputs": [],
   "source": [
    "#plot the histogram of learnt weights w_i \n",
    "plot_y_continous(reg.coef_, bins=20, title='Histogram of Parameters (w) learnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qBItenN5erIi",
   "metadata": {
    "id": "qBItenN5erIi"
   },
   "source": [
    "At this point, we can use our model to predict sentiments scores of tweets from `X_test`, i.e. test set. Do not forget to encode them as well.\n",
    "\n",
    "And save your predictions `y_hat` by naming it with the following format. \n",
    "\n",
    "`<TEAM_ID>__<SPLIT>_reg_pred.npy`\n",
    "\n",
    "Make sure that\n",
    "\n",
    "`<TEAM_ID>` is your team id as given in CMS.\n",
    "\n",
    "`<SPLIT>` is \"test_1\" during the semester and \"test_2\" for final submission. You will be notified when we need to move to \"test_2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee5e03",
   "metadata": {
    "id": "91ee5e03"
   },
   "outputs": [],
   "source": [
    "# Run this to save a file with your predictions on the test set to be submitted\n",
    "\n",
    "sentences = list(df_test.words_str.values)\n",
    "X_test = model.encode(sentences)\n",
    "y_hat = reg.predict(X_test)\n",
    "\n",
    "# Save the results with the format <TEAM_ID>__<SPLIT>_reg_pred.npy\n",
    "\n",
    "folder = 'result'\n",
    "np.save(os.path.join(folder, f'{team_id}__{split}__reg_pred.npy'), y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34410538",
   "metadata": {
    "id": "34410538"
   },
   "source": [
    "# Linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WeqFeth6hb6w",
   "metadata": {
    "id": "WeqFeth6hb6w"
   },
   "source": [
    "In this part, we will solve a linear classification task to predict our target `sentiment`, i.e. sentiment class of tweets, using our features which are encodings of the tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5ec07",
   "metadata": {
    "id": "7ad5ec07"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65482d95",
   "metadata": {
    "id": "65482d95"
   },
   "outputs": [],
   "source": [
    "def plot_y_discrete(y, show=True, title=None):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    sns.countplot(x=y, palette=colors, ax=ax)\n",
    "    if isinstance(title, str):\n",
    "        ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    if show: plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371be48e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1682351440534,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "371be48e",
    "outputId": "aeeb88b1-578e-421e-d471-8a379cb18f3c"
   },
   "outputs": [],
   "source": [
    "plot_y_discrete(df.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_QIyfxXthmTT",
   "metadata": {
    "id": "_QIyfxXthmTT"
   },
   "source": [
    "We will first change our targets (classes; positive, neutral, negative) to numeric targets. Then, we solve a logistic regression problem by minimizing the multinomial cross-entropy function\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{1}_{y_{i}=k} \\log(p_{\\theta}(\\hat{y}=k | \\mathbf{x}_{i}))\n",
    "$$\n",
    "\n",
    "where $y_i \\in \\{1,\\ldots,K\\}$ and $p_{\\theta}(\\hat{y}=k | \\mathbf{x}_{i})$ is the probability assigned by our model to class $k$ having observed features $\\mathbf{x}_{i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f516bc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1682351440534,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "5f516bc9",
    "outputId": "4991e414-ad05-44c8-bcf8-be32c14590e5"
   },
   "outputs": [],
   "source": [
    "X = sentence_embeddings\n",
    "y_text = df.sentiment.values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_text)\n",
    "print(f'Original classes {le.classes_}')\n",
    "print(f'Corresponding numeric classes {le.transform(le.classes_)}')\n",
    "y =le.transform(y_text)\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape} {np.unique(y)}\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "datasets = [\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val]\n",
    "]\n",
    "clf = linear_model.LogisticRegression(penalty='none', \n",
    "                                      dual=False, \n",
    "                                      tol=0.0001, \n",
    "                                      C=1.0, \n",
    "                                      fit_intercept=True, \n",
    "                                      intercept_scaling=1, \n",
    "                                      class_weight=None, # None, balanced\n",
    "                                      random_state=None, \n",
    "                                      solver='lbfgs', \n",
    "                                      max_iter=1000, \n",
    "                                      multi_class='auto', \n",
    "                                      verbose=0, \n",
    "                                      warm_start=False, \n",
    "                                      n_jobs=None, \n",
    "                                      l1_ratio=None\n",
    "                                      \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd0300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = linear_model.LogisticRegression(penalty='none', \n",
    "                                      dual=False, \n",
    "                                      tol=0.0001, \n",
    "                                      C=1.0, \n",
    "                                      fit_intercept=True, \n",
    "                                      intercept_scaling=1, \n",
    "                                      class_weight=None, # None, balanced\n",
    "                                      random_state=None, \n",
    "                                      solver='lbfgs', \n",
    "                                      max_iter=1000, \n",
    "                                      multi_class='auto', \n",
    "                                      verbose=0, \n",
    "                                      warm_start=False, \n",
    "                                      n_jobs=None, \n",
    "                                      l1_ratio=None\n",
    "                                      \n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af310057",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74732e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight = 'balanced', kernel = 'poly', coef0 = 1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114738e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(kf10, svc, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6932ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(kf10, svc, words12g75000_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aaa3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param = {\n",
    "#     'C': [1.0, 0.5, 0.1],\n",
    "    'kernel': ['poly'],\n",
    "#     'degree': [3],\n",
    "#     'gamma': ['scale', 'auto'],\n",
    "    'coef0': [0, 0.1, 1e-2, 1e-3],\n",
    "#     'tol': [1e-2, 1e-3, 1e-4],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = svc.fit(X_res, y_res)\n",
    "svc_gscv = GridSearchCV(svc, param_grid = svc_param, n_jobs = -1, cv = kf5, scoring = metrics.f1_score, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gscv = svc_gscv.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92beaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = SMOTE().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813df5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    [X_res, y_res],\n",
    "    [X_val, y_val]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80210292",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"X: {X_res.shape}\")\n",
    "print(f\"y: {y_res.shape} {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0S5pf_bA2XAV",
   "metadata": {
    "id": "0S5pf_bA2XAV"
   },
   "source": [
    "Fit your model by using training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167c0b4",
   "metadata": {
    "id": "c167c0b4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = clf.fit(X_res, y_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc_gscv.predict(X_val)\n",
    "rmse = np.sqrt(skm.mean_squared_error(y_val, y_pred))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e812a9",
   "metadata": {
    "id": "b4e812a9"
   },
   "source": [
    "\n",
    "Reminders about macro and micro averaging:\n",
    "\n",
    "\n",
    "In the context of computing F1-score, \"macro\" and \"micro\" averaging are two commonly used techniques to aggregate the per-class F1-scores.\n",
    "\n",
    "**Micro-average**: Compute the F1-score globally by counting the total true positives, false negatives, and false positives over all classes, and then calculating precision, recall, and F1-score using these aggregated values.\n",
    "\n",
    "**Macro-average**: Calculate the F1-score for each class separately, and then take the average of these per-class F1-scores.\n",
    "\n",
    "The main difference between these two techniques is the way they treat class imbalance. Micro-average treats all classes equally, regardless of their size, while macro-average treats each class equally, regardless of the number of samples in that class.\n",
    "\n",
    "Micro-average is often used when we care about overall performance across all classes, and we want to give more weight to the performance on larger classes. In contrast, macro-average is often used when we want to evaluate the performance on each class separately and give equal weight to each class.\n",
    "\n",
    "\n",
    "In addition to micro and macro averaging, there is another common technique for computing the F1-score called **weighted averaging**.\n",
    "\n",
    "**Weighted averaging** is similar to macro averaging in that it computes the per-class F1-score and then takes the average of these scores. However, unlike macro averaging, weighted averaging takes into account the number of samples in each class when computing the average. Specifically, the weighted average is computed as follows:\n",
    "\n",
    "- Compute the F1-score for each class separately.\n",
    "- Compute the weight for each class as the number of samples in that class divided by the total number of samples.\n",
    "- Compute the weighted average of the per-class F1-scores, where each per-class F1-score is weighted by the weight of that class.\n",
    "\n",
    "The weighted average is commonly used when the dataset is imbalanced, meaning that some classes have many more samples than others. In such cases, using the simple average (macro-average) would give too much weight to the smaller classes, while using micro-average would give too much weight to the larger classes. The weighted average strikes a balance between these two approaches by giving more weight to the classes with more samples while still taking into account the performance of all classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "URLzKp2J2x6i",
   "metadata": {
    "id": "URLzKp2J2x6i"
   },
   "source": [
    "Now evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e0e50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1682351440535,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "ff8e0e50",
    "outputId": "38afdacb-3bae-41a7-e1fb-b25dbbaf36c9"
   },
   "outputs": [],
   "source": [
    "for split_name, dataset in zip(['train', 'validation'], datasets):\n",
    "    X_i, y_i = dataset\n",
    "    y_pred = svc.predict(X_i)\n",
    "    print(f'\\nSplit: {split_name}')\n",
    "    print(skm.classification_report(y_i, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sGxdMSXO2-xH",
   "metadata": {
    "id": "sGxdMSXO2-xH"
   },
   "source": [
    "At this point, we can use our model to predict sentiments scores of tweets from `X_test`, i.e. test set. Do not forget to encode them as well.\n",
    "\n",
    "And save your predictions `y_hat` by naming it with the following format. \n",
    "\n",
    "`<TEAM_ID>__<SPLIT>_clf_pred.npy`\n",
    "\n",
    "Make sure that\n",
    "\n",
    "`<TEAM_ID>` is your team id as given in CMS.\n",
    "\n",
    "`<SPLIT>` is \"test_1\" during the semester and \"test_2\" for final submission. You will be notified when we need to move to \"test_2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cea870",
   "metadata": {
    "id": "d0cea870"
   },
   "outputs": [],
   "source": [
    "# Run this to save a file with your predictions on the test set to be submitted\n",
    "sentences = list(df_test.words_str.values)\n",
    "X_test = model.encode(sentences)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Save the results with the format <TEAM_ID>__<SPLIT>_clf_pred.npy\n",
    "\n",
    "folder = 'result'\n",
    "np.save(os.path.join(folder, f'{team_id}__{split}__clf_pred.npy'), y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CMofiwvW5Sd8",
   "metadata": {
    "id": "CMofiwvW5Sd8"
   },
   "source": [
    "# Submission to CMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n-7pww1o3iWN",
   "metadata": {
    "id": "n-7pww1o3iWN"
   },
   "source": [
    "Put your .npy files for both regression and classification tasks in the same zip file. Please name the file as `<TEAM_ID>.zip` and upload it to CMS system. It is essential that the files inside the .zip are named as follow:\n",
    "\n",
    "`<TEAM_ID>__<SPLIT>__reg_pred.npy` \\\n",
    "`<TEAM_ID>__<SPLIT>__clf_pred.npy` \\\n",
    "\n",
    "Above, `<SPLIT>` should correspond to `test_1` for the leaderboard and `test_2` for the final submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B75zOG1F-KAQ",
   "metadata": {
    "id": "B75zOG1F-KAQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1161689",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.load('result/1__test_1__reg_pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52b701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PyCharm (timung_sorting)",
   "language": "python",
   "name": "pycharm-ad6b22b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
